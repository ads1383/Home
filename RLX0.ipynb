{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MAI87oHtCKtu",
    "outputId": "7e6e55c8-a92f-4777-88ff-21e146464cae"
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "OZ0l9IvSWIAv",
    "outputId": "7d567537-8216-4b04-ec6f-a764742bd73c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
      "\u001b[K     |████████████████████████████████| 11.4MB 2.8MB/s \n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-1.9.6\n"
     ]
    }
   ],
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXJf0nDlCdeY"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "zl5s6XPGCqvm",
    "outputId": "79d65397-44ee-4865-f4f4-834718c85e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EmZ8xmaCKt8"
   },
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    def __init__(self,epsilon=0.2, alpha=0.3, gamma=0.9):\n",
    "        self.epsilon=epsilon\n",
    "        self.alpha=alpha\n",
    "        self.gamma=gamma\n",
    "        self.Q = {} #Q table\n",
    "        self.last_board=None\n",
    "        self.q_last=0.0\n",
    "        self.state_action_last=None\n",
    "\n",
    "    def game_begin(self):\n",
    "        self.last_board = None\n",
    "        self.q_last = 0.0\n",
    "        self.state_action_last = None\n",
    "\n",
    "\n",
    "    def epslion_greedy(self, state, possible_moves): #esplion greedy algorithm\n",
    "        #return  action\n",
    "        self.last_board = tuple(state)\n",
    "        if(random.random() < self.epsilon):\n",
    "            move = random.choice(possible_moves) ##action\n",
    "            self.state_action_last=(self.last_board,move)\n",
    "            self.q_last=self.getQ(self.last_board,move)\n",
    "            return move\n",
    "        else: #greedy strategy\n",
    "            Q_list=[]\n",
    "            for action in possible_moves:\n",
    "                Q_list.append(self.getQ(self.last_board,action))\n",
    "            maxQ=max(Q_list)\n",
    "\n",
    "            if Q_list.count(maxQ) > 1:\n",
    "                # more than 1 best option; choose among them randomly\n",
    "                best_options = [i for i in range(len(possible_moves)) if Q_list[i] == maxQ]\n",
    "                i = random.choice(best_options)\n",
    "            else:\n",
    "                i = Q_list.index(maxQ)\n",
    "            self.state_action_last = (self.last_board, possible_moves[i])\n",
    "            self.q_last = self.getQ(self.last_board, possible_moves[i])\n",
    "            return possible_moves[i]\n",
    "\n",
    "\n",
    "    def getQ(self, state, action): #get Q states\n",
    "        if(self.Q.get((state,action))) is None:\n",
    "            self.Q[(state,action)] = 1.0\n",
    "        return self.Q.get((state,action))\n",
    "\n",
    "    def updateQ(self, reward, state, possible_moves): # update Q states using Qleanning\n",
    "        q_list=[]\n",
    "        for moves in possible_moves:\n",
    "            q_list.append(self.getQ(tuple(state), moves))\n",
    "        if q_list:\n",
    "            max_q_next = max(q_list)\n",
    "        else:\n",
    "            max_q_next=0.0\n",
    "        self.Q[self.state_action_last] = self.q_last + self.alpha * ((reward + self.gamma*max_q_next) - self.q_last)\n",
    "\n",
    "    def saveQtable(self,file_name):  #save table\n",
    "        with open(file_name, 'wb') as handle:\n",
    "            pickle.dump(self.Q, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def loadQtable(self,file_name): # load table\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            self.Q = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdPQw6ZZCKuD"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    The Tic-Tac-Toe Policy\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=27, hidden_size=64, output_size=9):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        out = F.softmax(self.fc2(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oh6OTNBmCKuI"
   },
   "outputs": [],
   "source": [
    "def select_action(policy, state):\n",
    "    \"\"\"Samples an action from the policy at the state.\"\"\"\n",
    "    s = np.array([0]*9)\n",
    "    for i in range(len(state)):\n",
    "        if state[i] == ' ':\n",
    "            s[i] = 0\n",
    "        elif  state[i] == '0':\n",
    "            s[i] = 1\n",
    "        else:\n",
    "            s[i] = 2\n",
    "    state = s\n",
    "    state = torch.from_numpy(state).long().unsqueeze(0)\n",
    "    state = torch.zeros(3,9).scatter_(0,state,1).view(1,27)\n",
    "    pr = policy(Variable(state))\n",
    "    m = torch.distributions.Categorical(pr) \n",
    "    action = m.sample()\n",
    "    log_prob = torch.sum(m.log_prob(action))\n",
    "    \n",
    "    return action.data[0], log_prob \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUzl85dqCKuM"
   },
   "outputs": [],
   "source": [
    "def finish_episode(saved_rewards, saved_logprobs, gamma=1.0):\n",
    "    \"\"\"Samples an action from the policy at the state.\"\"\"\n",
    "    policy_loss = []\n",
    "    returns = compute_returns(saved_rewards, gamma)\n",
    "    returns = torch.Tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() +\n",
    "                                            np.finfo(np.float32).eps)\n",
    "  \n",
    "    for log_prob, reward in zip(saved_logprobs, returns):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "       \n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    # note: retain_graph=True allows for multiple calls to .backward()\n",
    "    # in a single step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmLHpv1aCKuV"
   },
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute returns for each time step, given the rewards\n",
    "      @param rewards: list of floats, where rewards[t] is the reward\n",
    "                      obtained at time step t\n",
    "      @param gamma: the discount factor\n",
    "      @returns list of floats representing the episode's returns\n",
    "          G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... \n",
    "\n",
    "    >>> compute_returns([0,0,0,1], 1.0)\n",
    "    [1.0, 1.0, 1.0, 1.0]\n",
    "    >>> compute_returns([0,0,0,1], 0.9)\n",
    "    [0.7290000000000001, 0.81, 0.9, 1.0]\n",
    "    >>> compute_returns([0,-0.5,5,0.5,-10], 0.9)\n",
    "    [-2.5965000000000003, -2.8850000000000002, -2.6500000000000004, -8.5, -10.0]\n",
    "    \"\"\"\n",
    "    r = []\n",
    "    x = 0\n",
    "    for i in range(len(rewards)):\n",
    "        x = rewards[i]\n",
    "        for j in range(0,len(rewards)-i):\n",
    "            if j!=0:\n",
    "                x= x + rewards[i + j]*gamma**j\n",
    "    \n",
    "        r.append(x)\n",
    "    return r    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8HNr1iCtpjj"
   },
   "outputs": [],
   "source": [
    "def load_weights(policy, episode = 170000):\n",
    "    \"\"\"Load saved weights\"\"\"\n",
    "    weights = torch.load(\"./RL/policy-%d.pkl\" % episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3uR2NjOCKuZ"
   },
   "outputs": [],
   "source": [
    "class Humanplayer:\n",
    "    pass\n",
    "\n",
    "#randomplayer player\n",
    "class Randomplayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def move(self,possiblemoves):\n",
    "        return random.choice(possiblemoves)\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self,traning=False):\n",
    "        self.board = [' ']*9\n",
    "\n",
    "        self.done = False\n",
    "        self.humman=None\n",
    "        self.computer=None\n",
    "        self.humanTurn=None\n",
    "        self.training=traning\n",
    "        self.player1 = None\n",
    "        self.player2 = None\n",
    "        self.aiplayer=None\n",
    "        self.isAI=False\n",
    "        # if not training display\n",
    "        if(not self.training):\n",
    "            pygame.init()\n",
    "            self.ttt = pygame.display.set_mode((225,250))\n",
    "            pygame.display.set_caption('Tic-Tac-Toe')\n",
    "\n",
    "    #reset the game\n",
    "    def reset(self):\n",
    "        if(self.training):\n",
    "            self.board = [' '] * 9\n",
    "            return \n",
    "\n",
    "        self.board = [' '] * 9\n",
    "        self.humanTurn=random.choice([True,False])\n",
    "\n",
    "        self.surface = pygame.Surface(self.ttt.get_size())\n",
    "        self.surface = self.surface.convert()\n",
    "        self.surface.fill((250, 250, 250))\n",
    "        #horizontal line\n",
    "        pygame.draw.line(self.surface, (0, 0, 0), (75, 0), (75, 225), 2)\n",
    "        pygame.draw.line(self.surface, (0, 0, 0), (150, 0), (150, 225), 2)\n",
    "        # veritical line\n",
    "        pygame.draw.line(self.surface, (0, 0, 0), (0,75), (225, 75), 2)\n",
    "        pygame.draw.line(self.surface, (0, 0, 0), (0,150), (225, 150), 2)\n",
    "\n",
    "   #evaluate function\n",
    "    def evaluate(self, ch):\n",
    "        # \"rows checking\"\n",
    "        for i in range(3):\n",
    "            if (ch == self.board[i * 3] == self.board[i * 3 + 1] and self.board[i * 3 + 1] == self.board[i * 3 + 2]):\n",
    "                return 1.0, True\n",
    "        # \"col checking\"\n",
    "        for i in range(3):\n",
    "            if (ch == self.board[i + 0] == self.board[i + 3] and self.board[i + 3] == self.board[i + 6]):\n",
    "                return 1.0, True\n",
    "        # diagonal checking\n",
    "        if (ch == self.board[0] == self.board[4] and self.board[4] == self.board[8]):\n",
    "            return 1.0, True\n",
    "\n",
    "        if (ch == self.board[2] == self.board[4] and self.board[4] == self.board[6]):\n",
    "            return 1.0, True\n",
    "        # \"if filled draw\"\n",
    "        if not any(c == ' ' for c in self.board):\n",
    "            return 0.5, True\n",
    "\n",
    "        return 0.0, False\n",
    "\n",
    "    #return remaining possible moves\n",
    "    def possible_moves(self):\n",
    "        return [moves + 1 for moves, v in enumerate(self.board) if v == ' ']\n",
    "\n",
    "    #take next step and return reward\n",
    "    def step(self, isX, move):\n",
    "        if(isX):\n",
    "             ch = 'X'\n",
    "        else:\n",
    "            ch = '0'\n",
    "        if(self.board[move-1]!=' '): # try to over write\n",
    "            return  -5.0, True\n",
    "\n",
    "        self.board[move-1]= ch\n",
    "        reward,done = self.evaluate(ch)\n",
    "        return reward, done\n",
    "\n",
    "\n",
    "    #draw move on window\n",
    "    def drawMove(self, pos,isX):\n",
    "        row=int((pos-1)/3)\n",
    "        col=(pos-1)%3\n",
    "\n",
    "        centerX = ((col) * 75) + 32\n",
    "        centerY = ((row) * 75) + 32\n",
    "\n",
    "        reward, done= self.step(isX,pos) #next step\n",
    "        if(reward==-5): #overlap\n",
    "            #print('Invalid move')\n",
    "            font = pygame.font.Font(None, 24)\n",
    "            text = font.render('Invalid move!', 1, (10, 10, 10))\n",
    "            self.surface.fill((250, 250, 250), (0, 300, 300, 25))\n",
    "            self.surface.blit(text, (10, 230))\n",
    "\n",
    "            return reward, done\n",
    "\n",
    "        if (isX): #playerX so draw x\n",
    "            font = pygame.font.Font(None, 24)\n",
    "            text = font.render('X', 1, (10, 10, 10))\n",
    "            self.surface.fill((250, 250, 250), (0, 300, 300, 25))\n",
    "            self.surface.blit(text, (centerX, centerY))\n",
    "            self.board[pos-1] ='X'\n",
    "\n",
    "            if(self.humman and reward==1): #if playerX is humman and won, display humman won\n",
    "                #print('Humman won! in X')\n",
    "                text = font.render('Humman won!', 1, (10, 10, 10))\n",
    "                self.surface.fill((250, 250, 250), (0, 300, 300, 25))\n",
    "                self.surface.blit(text, (10, 230))\n",
    "\n",
    "\n",
    "            elif (self.computer and reward == 1):#if playerX is computer and won, display computer won\n",
    "                #print('computer won! in X')\n",
    "                text = font.render('computer won!', 1, (10, 10, 10))\n",
    "                self.surface.fill((250, 250, 250), (0, 300, 300, 25))\n",
    "                self.surface.blit(text, (10, 230))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        else:  #playerO so draw O\n",
    "            font = pygame.font.Font(None, 24)\n",
    "            text = font.render('O', 1, (10, 10, 10))\n",
    "\n",
    "            self.surface.fill((250, 250, 250), (0, 300, 300, 25))\n",
    "            self.surface.blit(text, (centerX, centerY))\n",
    "            self.board[pos-1] = '0'\n",
    "\n",
    "            if (not self.humman and reward == 1):  #if playerO is humman and won, display humman won\n",
    "                #print('Humman won! in O')\n",
    "                text = font.render('Humman won!', 1, (10, 10, 10))\n",
    "                self.surface.fill((250, 250, 250), (0, 300, 300, 25))\n",
    "                self.surface.blit(text, (10, 230))\n",
    "\n",
    "\n",
    "            elif (not self.computer and reward == 1):  #if playerO is computer and won, display computer won\n",
    "                #print('computer won! in O')\n",
    "                text = font.render('computer won!', 1, (10, 10, 10))\n",
    "                self.surface.fill((250, 250, 250), (0, 300, 300, 25))\n",
    "                self.surface.blit(text, (10, 230))\n",
    "\n",
    "\n",
    "\n",
    "        if (reward == 0.5):  # draw, then display draw\n",
    "            #print('Draw Game! in O')\n",
    "            font = pygame.font.Font(None, 24)\n",
    "            text = font.render('Draw Game!', 1, (10, 10, 10))\n",
    "            self.surface.fill((250, 250, 250), (0, 300, 300, 25))\n",
    "            self.surface.blit(text, (10, 230))\n",
    "            return reward, done\n",
    "\n",
    "        return reward,done\n",
    "\n",
    "    # mouseClick position\n",
    "    def mouseClick(self):\n",
    "        (mouseX, mouseY) = pygame.mouse.get_pos()\n",
    "        if (mouseY < 75):\n",
    "            row = 0\n",
    "        elif (mouseY < 150):\n",
    "            row = 1\n",
    "        else:\n",
    "            row = 2\n",
    "\n",
    "        if (mouseX < 75):\n",
    "            col = 0\n",
    "        elif (mouseX < 150):\n",
    "            col = 1\n",
    "        else:\n",
    "            col = 2\n",
    "        return row * 3 + col + 1\n",
    "\n",
    "\n",
    "     #update state\n",
    "    def updateState(self,isX):\n",
    "        pos=self.mouseClick()\n",
    "        reward,done = self.drawMove(pos,isX)\n",
    "        return reward, done\n",
    "\n",
    "    #show display\n",
    "    def showboard(self):\n",
    "        self.ttt.blit(self.surface, (0, 0))\n",
    "        pygame.display.flip()\n",
    "\n",
    "\n",
    "    #begin training\n",
    "    def startTraining(self,player1,player2):\n",
    "        if(isinstance(player1,Qlearning) and isinstance(player2, Qlearning)):   \n",
    "        #if(isinstance(player1,Qlearning) and isinstance(player2, Qlearning)):\n",
    "            self.training = True\n",
    "            self.player1=player1\n",
    "            self.player2=player2\n",
    "\n",
    "        elif(isinstance(player1,Qlearning) and isinstance(player2, Policy)):   \n",
    "        #if(isinstance(player1,Qlearning) and isinstance(player2, Qlearning)):\n",
    "            self.training = True\n",
    "            self.player1=player1\n",
    "            self.player2=player2\n",
    "        elif(isinstance(player1,Policy) and isinstance(player2, Policy)):   \n",
    "        #if(isinstance(player1,Qlearning) and isinstance(player2, Qlearning)):\n",
    "            self.training = True\n",
    "            self.player1=player1\n",
    "            self.player2=player2\n",
    "        elif(isinstance(player1,Policy) and isinstance(player2, Qlearning)):   \n",
    "        #if(isinstance(player1,Qlearning) and isinstance(player2, Qlearning)):\n",
    "            self.training = True\n",
    "            self.player1=player1\n",
    "            self.player2=player2\n",
    "   \n",
    "    #tarin function\n",
    "    def train(self,iterations, log_interval = 10000):\n",
    "        if(self.training):\n",
    "           \n",
    "            if isinstance(self.player1, Policy):\n",
    "                    optimizer1 = optim.Adam(self.player1.parameters(), lr=0.001)\n",
    "                    scheduler1 = torch.optim.lr_scheduler.StepLR(optimizer1, step_size=10000, gamma=0.95)\n",
    "                    running_reward1 = 0\n",
    "                  \n",
    "            if isinstance(self.player2, Policy):\n",
    "                    optimizer2 = optim.Adam(self.player2.parameters(), lr=0.001)\n",
    "                    scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer2, step_size=10000, gamma=0.95)\n",
    "                    running_reward2 = 0\n",
    "                    \n",
    "            \n",
    "            for i in range(1,iterations):\n",
    "                if i % 5000 == 0:\n",
    "                    print(\"trainining\", i)\n",
    "                \n",
    "                if isinstance(self.player1, Policy):\n",
    "                    saved_rewards1 = [0]\n",
    "                    saved_logprobs1 = []\n",
    "                    optimizer1.zero_grad()\n",
    "                \n",
    "                else:\n",
    "                    self.player1.game_begin()\n",
    "                  \n",
    "                if isinstance(self.player2, Policy):\n",
    "                    saved_rewards2 = [0]\n",
    "                    saved_logprobs2 = []\n",
    "                    optimizer2.zero_grad()\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    self.player2.game_begin()\n",
    "                \n",
    "                self.reset()\n",
    "                done = False\n",
    "                isX = random.choice([True, False])\n",
    "                while not done:\n",
    "                    if isX:\n",
    "                        if isinstance(self.player1, Policy):\n",
    "                            state = self.board\n",
    "                            move, logprob = select_action(Policy, state)\n",
    "                            saved_logprobs1.append(logprob)\n",
    "                            move = move+1\n",
    "                        else:\n",
    "                            move = self.player1.epslion_greedy(self.board, self.possible_moves())\n",
    "                                             \n",
    "                    else:\n",
    "                        if isinstance(self.player2, Policy):\n",
    "                            state = self.board\n",
    "                            move, logprob = select_action(self.player2, state)\n",
    "                            saved_logprobs2.append(logprob)\n",
    "                            move = move+1\n",
    "                           \n",
    "                        else:\n",
    "                            move = self.player2.epslion_greedy(self.board, self.possible_moves())\n",
    "\n",
    "\n",
    "                    reward, done = self.step(isX, move)\n",
    "                   \n",
    "                    if (reward == 1):  # won\n",
    "                        if (isX):\n",
    "                            if isinstance(self.player1, Policy):\n",
    "                                saved_rewards1.append(reward)\n",
    "                            else:          \n",
    "                                self.player1.updateQ(reward, self.board, self.possible_moves())\n",
    "                            if isinstance(self.player2, Policy):\n",
    "                                saved_rewards2.append(-1*reward)\n",
    "                            else:\n",
    "                                self.player2.updateQ(-1 * reward, self.board, self.possible_moves())\n",
    "                        else:\n",
    "                            if isinstance(self.player1, Policy):\n",
    "                                saved_rewards1.append(-1*reward)\n",
    "                            else:          \n",
    "                                self.player1.updateQ(-1*reward, self.board, self.possible_moves())\n",
    "                            if isinstance(self.player2, Policy):\n",
    "                                saved_rewards2.append(reward)\n",
    "                            else:\n",
    "                                self.player2.updateQ(reward, self.board, self.possible_moves())\n",
    "                            \n",
    "                    elif (reward == 0.5):  # draw\n",
    "                            if isinstance(self.player1, Policy):\n",
    "                                saved_rewards1.append(reward)\n",
    "                            else:          \n",
    "                                self.player1.updateQ(reward, self.board, self.possible_moves())\n",
    "                            if isinstance(self.player2, Policy):\n",
    "                                saved_rewards2.append(reward)\n",
    "                            else:\n",
    "                                self.player2.updateQ(reward, self.board, self.possible_moves())\n",
    "                        \n",
    "\n",
    "\n",
    "                    elif (reward == -5):  # illegal move\n",
    "                        if (isX):\n",
    "                            if isinstance(self.player1, Policy):\n",
    "                                saved_rewards1.append(reward)\n",
    "                            else:          \n",
    "                                self.player1.updateQ(reward, self.board, self.possible_moves())\n",
    "                            \n",
    "                            \n",
    "                        else:\n",
    "                            if isinstance(self.player2, Policy):\n",
    "                                saved_rewards2.append(reward)\n",
    "                            else:          \n",
    "                                self.player2.updateQ(reward, self.board, self.possible_moves())\n",
    "                            \n",
    "                    elif (reward == 0):\n",
    "                        if (isX):  # update opposite\n",
    "                            if isinstance(self.player1, Policy):\n",
    "                                saved_rewards1.append(reward)\n",
    "                            else:          \n",
    "                                self.player1.updateQ(reward, self.board, self.possible_moves())\n",
    "                            \n",
    "                        else:\n",
    "                            if isinstance(self.player2, Policy):\n",
    "                                saved_rewards2.append(reward)\n",
    "                            else:          \n",
    "                                self.player2.updateQ(reward, self.board, self.possible_moves())\n",
    "                           \n",
    "                   \n",
    "                    isX = not isX  #\n",
    "                    \n",
    "                if isinstance(self.player1, Policy):\n",
    "                    R = compute_returns(saved_rewards1)[0]\n",
    "                    running_reward1 += R\n",
    "                    \n",
    "                    finish_episode(saved_rewards1, saved_logprobs1, gamma=0.98)\n",
    "        \n",
    "        \n",
    "                    if i % log_interval == 0:\n",
    "                       print('Episode {}\\tAverage return player1: {:.2f}'.format(\n",
    "                                i,\n",
    "                                running_reward1 / log_interval))\n",
    "                       running_reward1 = 0\n",
    "\n",
    "                    if i % (log_interval) == 0:\n",
    "                        torch.save(player1.state_dict(),\n",
    "                                 \"./RL/player1-%d.pkl\" % i)\n",
    "\n",
    "                    if i % 16 == 0: # batch_size\n",
    "                        \n",
    "                        optimizer1.step()\n",
    "                        scheduler1.step()\n",
    "                        optimizer1.zero_grad()\n",
    "                    \n",
    "                if isinstance(self.player2, Policy):\n",
    "                    R = compute_returns(saved_rewards2)[0]\n",
    "                    running_reward2 += R\n",
    "             \n",
    "                    finish_episode(saved_rewards2, saved_logprobs2, gamma=0.98)\n",
    "        \n",
    "        \n",
    "                    if i % log_interval == 0:\n",
    "                       print('Episode {}\\tAverage return player2: {:.2f}'.format(\n",
    "                            i,\n",
    "                            running_reward2 / log_interval))\n",
    "                       running_reward2 = 0\n",
    "                       \n",
    "                    if i % (log_interval) == 0:\n",
    "                        torch.save(player2.state_dict(),\n",
    "                                 \"./RL/player2-%d.pkl\" % i)\n",
    "                      \n",
    "                    if i % 16 == 0: # batch_size\n",
    "                        \n",
    "                        optimizer2.step()\n",
    "                        scheduler2.step()\n",
    "                        optimizer2.zero_grad()\n",
    "                        \n",
    "    #save Qtables\n",
    "    def saveStates(self):\n",
    "        if isinstance(self.player1, Qlearning):\n",
    "            self.player1.saveQtable(\"player1states\")\n",
    "        if isinstance(self.player2, Qlearning):\n",
    "            self.player2.saveQtable(\"player2states\")\n",
    "\n",
    "\n",
    "    #start game human vs AI or human vs random\n",
    "    def startGame(self, playerX, playerO):\n",
    "        if isinstance(playerX, Humanplayer):\n",
    "            self.humman, self.computer = True, False\n",
    "            if isinstance(playerO, Qlearning): #if AI\n",
    "                self.ai = playerO\n",
    "                self.ai.loadQtable(\"player2states\") # load saved Q table\n",
    "                self.ai.epsilon = 0 #set eps to 0 so always choose greedy step\n",
    "                self.isAI = True\n",
    "            elif isinstance(playerO, Randomplayer): #if random\n",
    "                self.ai = playerO\n",
    "                self.isAI = False\n",
    "            elif isinstance(playerO, Policy):\n",
    "                self.ai = playerO\n",
    "                load_weights(self.ai)\n",
    "                self.ai.epsilon = 0 #set eps to 0 so always choose greedy step\n",
    "                self.isAI = True\n",
    "                \n",
    "        elif isinstance(playerO, Humanplayer):\n",
    "            self.humman, self.computer = False, True\n",
    "            if isinstance(playerX, Qlearning): #if AI\n",
    "                self.ai = playerX\n",
    "                self.ai.loadQtable(\"player1states\") # load saved Q table\n",
    "                self.ai.epsilon = 0 #set eps to 0 so always choose greedy step\n",
    "                self.isAI = True\n",
    "            elif isinstance(playerX, Randomplayer):#if random\n",
    "                self.ai=playerX\n",
    "                self.isAI = False\n",
    "            elif isinstance(playerX, Policy):\n",
    "                self.ai = playerX\n",
    "                load_weights(self.ai)\n",
    "                self.ai.epsilon = 0 #set eps to 0 so always choose greedy step\n",
    "                self.isAI = True\n",
    "\n",
    "    def render(self):\n",
    "        running = 1\n",
    "        done = False\n",
    "        pygame.event.clear()\n",
    "        while (running == 1):\n",
    "            if (self.humanTurn): #humman click\n",
    "                print(\"Human player turn\")\n",
    "                event = pygame.event.wait()\n",
    "                while event.type != pygame.MOUSEBUTTONDOWN:\n",
    "                    event = pygame.event.wait()\n",
    "                    self.showboard()\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        running = 0\n",
    "                        print(\"pressed quit\")\n",
    "                        break\n",
    "\n",
    "                reward, done = self.updateState(self.humman) #if random\n",
    "                self.showboard()\n",
    "                if (done): #if done reset\n",
    "                    time.sleep(1)\n",
    "                    self.reset()\n",
    "            else:  #AI or random turn\n",
    "                if self.isAI:\n",
    "                    \n",
    "                    if isinstance(self.ai, Policy):\n",
    "                        move, _ = select_action(self.ai, self.board)\n",
    "                        moves = move+1\n",
    "                        reward, done = self.drawMove(moves, self.computer)\n",
    "                    else:\n",
    "                        moves = self.ai.epslion_greedy(self.board, self.possible_moves())\n",
    "                        reward, done = self.drawMove(moves, self.computer)\n",
    "                    print(\"computer's AI player turn\")\n",
    "                    self.showboard()\n",
    "                else: #random player\n",
    "                    moves = self.ai.move(self.possible_moves()) #random player\n",
    "                    reward, done = self.drawMove(moves, self.computer)\n",
    "                    print(\"computer's random player turn\")\n",
    "                    self.showboard()\n",
    "\n",
    "                if (done): #if done reset\n",
    "                    time.sleep(1)\n",
    "                    self.reset()\n",
    "\n",
    "            self.humanTurn = not self.humanTurn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "LzT3FCapEQiM",
    "outputId": "0364c548-dc67-4939-cec1-f798ff2c3615",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainining 5000\n",
      "trainining 10000\n",
      "Episode 10000\tAverage return player2: -3.91\n",
      "trainining 15000\n",
      "trainining 20000\n",
      "Episode 20000\tAverage return player2: -3.46\n",
      "trainining 25000\n",
      "trainining 30000\n",
      "Episode 30000\tAverage return player2: -3.09\n",
      "trainining 35000\n",
      "trainining 40000\n",
      "Episode 40000\tAverage return player2: -2.83\n",
      "trainining 45000\n",
      "trainining 50000\n",
      "Episode 50000\tAverage return player2: -2.38\n",
      "trainining 55000\n",
      "trainining 60000\n",
      "Episode 60000\tAverage return player2: -2.03\n",
      "trainining 65000\n",
      "trainining 70000\n",
      "Episode 70000\tAverage return player2: -1.86\n",
      "trainining 75000\n",
      "trainining 80000\n",
      "Episode 80000\tAverage return player2: -1.78\n",
      "trainining 85000\n",
      "trainining 90000\n",
      "Episode 90000\tAverage return player2: -1.50\n",
      "trainining 95000\n",
      "trainining 100000\n",
      "Episode 100000\tAverage return player2: -1.48\n",
      "trainining 105000\n",
      "trainining 110000\n",
      "Episode 110000\tAverage return player2: -1.40\n",
      "trainining 115000\n",
      "trainining 120000\n",
      "Episode 120000\tAverage return player2: -1.33\n",
      "trainining 125000\n",
      "trainining 130000\n",
      "Episode 130000\tAverage return player2: -1.32\n",
      "trainining 135000\n",
      "trainining 140000\n",
      "Episode 140000\tAverage return player2: -1.37\n",
      "trainining 145000\n",
      "trainining 150000\n",
      "Episode 150000\tAverage return player2: -1.36\n",
      "trainining 155000\n",
      "trainining 160000\n",
      "Episode 160000\tAverage return player2: -1.31\n",
      "trainining 165000\n",
      "trainining 170000\n",
      "Episode 170000\tAverage return player2: -1.13\n",
      "trainining 175000\n",
      "trainining 180000\n",
      "Episode 180000\tAverage return player2: -1.06\n",
      "trainining 185000\n",
      "trainining 190000\n",
      "Episode 190000\tAverage return player2: -0.95\n",
      "trainining 195000\n",
      "trainining 200000\n",
      "Episode 200000\tAverage return player2: -1.05\n",
      "trainining 205000\n",
      "trainining 210000\n",
      "Episode 210000\tAverage return player2: -1.09\n",
      "trainining 215000\n",
      "trainining 220000\n",
      "Episode 220000\tAverage return player2: -1.26\n",
      "trainining 225000\n",
      "trainining 230000\n",
      "Episode 230000\tAverage return player2: -1.04\n",
      "trainining 235000\n",
      "trainining 240000\n",
      "Episode 240000\tAverage return player2: -1.01\n",
      "trainining 245000\n",
      "trainining 250000\n",
      "Episode 250000\tAverage return player2: -1.01\n",
      "trainining 255000\n",
      "trainining 260000\n",
      "Episode 260000\tAverage return player2: -1.12\n",
      "trainining 265000\n",
      "trainining 270000\n",
      "Episode 270000\tAverage return player2: -1.11\n",
      "trainining 275000\n",
      "trainining 280000\n",
      "Episode 280000\tAverage return player2: -1.10\n",
      "trainining 285000\n",
      "trainining 290000\n",
      "Episode 290000\tAverage return player2: -0.99\n",
      "trainining 295000\n",
      "trainining 300000\n",
      "Episode 300000\tAverage return player2: -0.94\n",
      "trainining 305000\n",
      "trainining 310000\n",
      "Episode 310000\tAverage return player2: -1.04\n",
      "trainining 315000\n",
      "trainining 320000\n",
      "Episode 320000\tAverage return player2: -1.02\n",
      "trainining 325000\n",
      "trainining 330000\n",
      "Episode 330000\tAverage return player2: -1.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ad2038680e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplayer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#player2 learning agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplayer2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#train for 200,000 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveStates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#save Qtable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-1ed9779c4357>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, iterations, log_interval)\u001b[0m\n\u001b[1;32m    268\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                             \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m                             \u001b[0msaved_logprobs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                             \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-39f06310ceee>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(policy, state)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game = TicTacToe(True) #game instance, True means training\n",
    "player1= Qlearning() #player1 learning agent \n",
    "player2 = Policy() #player2 learning agent \n",
    "game.startTraining(player1,player2) #start training\n",
    "game.train(500000, log_interval=10000) #train for 200,000 iterations\n",
    "game.saveStates()  #save Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhS64hC7qwAm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "iYC3Cc-ZCKuf",
    "outputId": "f35ba7fa-3b67-4cb9-f627-e61dddd32e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human player turn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n",
      "computer's AI player turn\n",
      "Human player turn\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0b5f44a7b3d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplayer2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#player1 is X, player2 is 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# render display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-ce0873fdc9e5>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMOUSEBUTTONDOWN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game = TicTacToe() #game instance\n",
    "player1=Humanplayer() #human player\n",
    "player2=Policy()  #agent\n",
    "game.startGame(player1,player2)#player1 is X, player2 is 0\n",
    "game.reset() #reset\n",
    "game.render() # render display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2FWVkONCKun"
   },
   "outputs": [],
   "source": [
    "game = TicTacToe() #game instance\n",
    "player1=Humanplayer() #human player\n",
    "player2=Qlearning()  #agent\n",
    "game.startGame(player1,player2)#player1 is X, player2 is 0\n",
    "game.reset() #reset\n",
    "game.render() # render display"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RL2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
