{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import wiki_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, TEXT = wiki_utils.WikiTexts(batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "       \n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = weight_matrix.size(0)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, text in enumerate(data_loader):\n",
    "        data, targets = text.text, text.target\n",
    "   #     output, hidden = model(data)\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "   #     total_loss += len(data) * criterion(output_flat, targets.view(-1)).item()\n",
    "        total_loss += criterion(output_flat, targets.view(-1)).item()\n",
    "    return total_loss/len(data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = weight_matrix.size(0)\n",
    "    for batch, text in enumerate(train_loader):\n",
    "        data, targets = text.text, text.target\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_loader), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = weight_matrix.size(0)\n",
    "model = RNNModel('LSTM', ntokens, weight_matrix.size(1), weight_matrix.size(1), 2, 0.3)\n",
    "model.encoder.weight.data.copy_(weight_matrix)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()    \n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]       \n",
    "        x.data.fill_(s_idx)             \n",
    "        s = TEXT.vocab.itos[s_idx]                    \n",
    "        out.append(s)\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " leaned principals naoko themes torpedo afrodisiac honoured khalid towards gallons full sankara donors falsetto larva reprises symbols knott insurance tiered \n",
      "\n",
      "| epoch   1 |   100/ 2330 batches | lr 4.00 | loss  7.93 | ppl  2775.82\n",
      "| epoch   1 |   200/ 2330 batches | lr 4.00 | loss  7.10 | ppl  1206.98\n",
      "| epoch   1 |   300/ 2330 batches | lr 4.00 | loss  6.88 | ppl   970.26\n",
      "| epoch   1 |   400/ 2330 batches | lr 4.00 | loss  6.72 | ppl   825.69\n",
      "| epoch   1 |   500/ 2330 batches | lr 4.00 | loss  6.62 | ppl   748.54\n",
      "| epoch   1 |   600/ 2330 batches | lr 4.00 | loss  6.52 | ppl   679.95\n",
      "| epoch   1 |   700/ 2330 batches | lr 4.00 | loss  6.49 | ppl   655.71\n",
      "| epoch   1 |   800/ 2330 batches | lr 4.00 | loss  6.53 | ppl   688.09\n",
      "| epoch   1 |   900/ 2330 batches | lr 4.00 | loss  6.41 | ppl   606.34\n",
      "| epoch   1 |  1000/ 2330 batches | lr 4.00 | loss  6.39 | ppl   593.62\n",
      "| epoch   1 |  1100/ 2330 batches | lr 4.00 | loss  6.42 | ppl   616.90\n",
      "| epoch   1 |  1200/ 2330 batches | lr 4.00 | loss  6.39 | ppl   594.19\n",
      "| epoch   1 |  1300/ 2330 batches | lr 4.00 | loss  6.32 | ppl   557.72\n",
      "| epoch   1 |  1400/ 2330 batches | lr 4.00 | loss  6.30 | ppl   542.29\n",
      "| epoch   1 |  1500/ 2330 batches | lr 4.00 | loss  6.30 | ppl   544.96\n",
      "| epoch   1 |  1600/ 2330 batches | lr 4.00 | loss  6.27 | ppl   526.32\n",
      "| epoch   1 |  1700/ 2330 batches | lr 4.00 | loss  6.28 | ppl   536.27\n",
      "| epoch   1 |  1800/ 2330 batches | lr 4.00 | loss  6.24 | ppl   514.84\n",
      "| epoch   1 |  1900/ 2330 batches | lr 4.00 | loss  6.21 | ppl   499.90\n",
      "| epoch   1 |  2000/ 2330 batches | lr 4.00 | loss  6.22 | ppl   500.49\n",
      "| epoch   1 |  2100/ 2330 batches | lr 4.00 | loss  6.25 | ppl   518.30\n",
      "| epoch   1 |  2200/ 2330 batches | lr 4.00 | loss  6.20 | ppl   493.76\n",
      "| epoch   1 |  2300/ 2330 batches | lr 4.00 | loss  6.12 | ppl   454.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  5.59 | valid ppl   267.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " and eye hayhurst festival , stowage furthermore in the move of nonetheless works he major @-@ their same heels way \n",
      "\n",
      "| epoch   2 |   100/ 2330 batches | lr 4.00 | loss  6.20 | ppl   493.02\n",
      "| epoch   2 |   200/ 2330 batches | lr 4.00 | loss  6.08 | ppl   435.92\n",
      "| epoch   2 |   300/ 2330 batches | lr 4.00 | loss  6.08 | ppl   438.26\n",
      "| epoch   2 |   400/ 2330 batches | lr 4.00 | loss  6.09 | ppl   443.59\n",
      "| epoch   2 |   500/ 2330 batches | lr 4.00 | loss  6.05 | ppl   425.34\n",
      "| epoch   2 |   600/ 2330 batches | lr 4.00 | loss  6.01 | ppl   409.25\n",
      "| epoch   2 |   700/ 2330 batches | lr 4.00 | loss  6.01 | ppl   407.65\n",
      "| epoch   2 |   800/ 2330 batches | lr 4.00 | loss  6.09 | ppl   440.92\n",
      "| epoch   2 |   900/ 2330 batches | lr 4.00 | loss  6.01 | ppl   407.96\n",
      "| epoch   2 |  1000/ 2330 batches | lr 4.00 | loss  6.01 | ppl   406.89\n",
      "| epoch   2 |  1100/ 2330 batches | lr 4.00 | loss  6.06 | ppl   427.21\n",
      "| epoch   2 |  1200/ 2330 batches | lr 4.00 | loss  6.04 | ppl   418.02\n",
      "| epoch   2 |  1300/ 2330 batches | lr 4.00 | loss  6.00 | ppl   401.68\n",
      "| epoch   2 |  1400/ 2330 batches | lr 4.00 | loss  5.98 | ppl   395.72\n",
      "| epoch   2 |  1500/ 2330 batches | lr 4.00 | loss  6.00 | ppl   403.68\n",
      "| epoch   2 |  1600/ 2330 batches | lr 4.00 | loss  5.97 | ppl   389.71\n",
      "| epoch   2 |  1700/ 2330 batches | lr 4.00 | loss  5.99 | ppl   399.60\n",
      "| epoch   2 |  1800/ 2330 batches | lr 4.00 | loss  5.95 | ppl   384.76\n",
      "| epoch   2 |  1900/ 2330 batches | lr 4.00 | loss  5.95 | ppl   383.14\n",
      "| epoch   2 |  2000/ 2330 batches | lr 4.00 | loss  5.95 | ppl   385.23\n",
      "| epoch   2 |  2100/ 2330 batches | lr 4.00 | loss  5.99 | ppl   399.25\n",
      "| epoch   2 |  2200/ 2330 batches | lr 4.00 | loss  5.96 | ppl   387.09\n",
      "| epoch   2 |  2300/ 2330 batches | lr 4.00 | loss  5.89 | ppl   362.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  5.38 | valid ppl   216.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " across officially an pays filled that it are rickshaws , like social game with the remembered rodrigues pushing by sundance \n",
      "\n",
      "| epoch   3 |   100/ 2330 batches | lr 4.00 | loss  5.99 | ppl   398.58\n",
      "| epoch   3 |   200/ 2330 batches | lr 4.00 | loss  5.87 | ppl   354.87\n",
      "| epoch   3 |   300/ 2330 batches | lr 4.00 | loss  5.87 | ppl   355.04\n",
      "| epoch   3 |   400/ 2330 batches | lr 4.00 | loss  5.90 | ppl   363.93\n",
      "| epoch   3 |   500/ 2330 batches | lr 4.00 | loss  5.86 | ppl   351.37\n",
      "| epoch   3 |   600/ 2330 batches | lr 4.00 | loss  5.83 | ppl   341.69\n",
      "| epoch   3 |   700/ 2330 batches | lr 4.00 | loss  5.83 | ppl   339.46\n",
      "| epoch   3 |   800/ 2330 batches | lr 4.00 | loss  5.91 | ppl   367.63\n",
      "| epoch   3 |   900/ 2330 batches | lr 4.00 | loss  5.84 | ppl   342.96\n",
      "| epoch   3 |  1000/ 2330 batches | lr 4.00 | loss  5.84 | ppl   342.46\n",
      "| epoch   3 |  1100/ 2330 batches | lr 4.00 | loss  5.89 | ppl   361.19\n",
      "| epoch   3 |  1200/ 2330 batches | lr 4.00 | loss  5.87 | ppl   355.31\n",
      "| epoch   3 |  1300/ 2330 batches | lr 4.00 | loss  5.83 | ppl   342.00\n",
      "| epoch   3 |  1400/ 2330 batches | lr 4.00 | loss  5.82 | ppl   338.46\n",
      "| epoch   3 |  1500/ 2330 batches | lr 4.00 | loss  5.85 | ppl   346.09\n",
      "| epoch   3 |  1600/ 2330 batches | lr 4.00 | loss  5.81 | ppl   334.56\n",
      "| epoch   3 |  1700/ 2330 batches | lr 4.00 | loss  5.84 | ppl   342.89\n",
      "| epoch   3 |  1800/ 2330 batches | lr 4.00 | loss  5.80 | ppl   328.73\n",
      "| epoch   3 |  1900/ 2330 batches | lr 4.00 | loss  5.80 | ppl   330.77\n",
      "| epoch   3 |  2000/ 2330 batches | lr 4.00 | loss  5.81 | ppl   333.40\n",
      "| epoch   3 |  2100/ 2330 batches | lr 4.00 | loss  5.84 | ppl   344.73\n",
      "| epoch   3 |  2200/ 2330 batches | lr 4.00 | loss  5.81 | ppl   335.00\n",
      "| epoch   3 |  2300/ 2330 batches | lr 4.00 | loss  5.76 | ppl   318.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  5.25 | valid ppl   190.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " further its make . she said also he never like him zhou be can near hemlock hereford , but that \n",
      "\n",
      "| epoch   4 |   100/ 2330 batches | lr 4.00 | loss  5.85 | ppl   348.85\n",
      "| epoch   4 |   200/ 2330 batches | lr 4.00 | loss  5.74 | ppl   312.55\n",
      "| epoch   4 |   300/ 2330 batches | lr 4.00 | loss  5.75 | ppl   312.78\n",
      "| epoch   4 |   400/ 2330 batches | lr 4.00 | loss  5.78 | ppl   323.61\n",
      "| epoch   4 |   500/ 2330 batches | lr 4.00 | loss  5.74 | ppl   311.65\n",
      "| epoch   4 |   600/ 2330 batches | lr 4.00 | loss  5.72 | ppl   303.93\n",
      "| epoch   4 |   700/ 2330 batches | lr 4.00 | loss  5.70 | ppl   300.27\n",
      "| epoch   4 |   800/ 2330 batches | lr 4.00 | loss  5.78 | ppl   324.57\n",
      "| epoch   4 |   900/ 2330 batches | lr 4.00 | loss  5.72 | ppl   305.76\n",
      "| epoch   4 |  1000/ 2330 batches | lr 4.00 | loss  5.72 | ppl   304.91\n",
      "| epoch   4 |  1100/ 2330 batches | lr 4.00 | loss  5.77 | ppl   321.24\n",
      "| epoch   4 |  1200/ 2330 batches | lr 4.00 | loss  5.76 | ppl   317.18\n",
      "| epoch   4 |  1300/ 2330 batches | lr 4.00 | loss  5.72 | ppl   305.91\n",
      "| epoch   4 |  1400/ 2330 batches | lr 4.00 | loss  5.72 | ppl   305.18\n",
      "| epoch   4 |  1500/ 2330 batches | lr 4.00 | loss  5.74 | ppl   310.82\n",
      "| epoch   4 |  1600/ 2330 batches | lr 4.00 | loss  5.71 | ppl   301.16\n",
      "| epoch   4 |  1700/ 2330 batches | lr 4.00 | loss  5.73 | ppl   307.50\n",
      "| epoch   4 |  1800/ 2330 batches | lr 4.00 | loss  5.68 | ppl   294.18\n",
      "| epoch   4 |  1900/ 2330 batches | lr 4.00 | loss  5.70 | ppl   297.79\n",
      "| epoch   4 |  2000/ 2330 batches | lr 4.00 | loss  5.71 | ppl   300.50\n",
      "| epoch   4 |  2100/ 2330 batches | lr 4.00 | loss  5.74 | ppl   312.23\n",
      "| epoch   4 |  2200/ 2330 batches | lr 4.00 | loss  5.71 | ppl   303.01\n",
      "| epoch   4 |  2300/ 2330 batches | lr 4.00 | loss  5.67 | ppl   289.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  5.17 | valid ppl   175.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " cross wrote shizhong as the new duchovny . \" <eos>   the wolfgang play their a 1970s of mohamed for \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |   100/ 2330 batches | lr 4.00 | loss  5.76 | ppl   317.74\n",
      "| epoch   5 |   200/ 2330 batches | lr 4.00 | loss  5.65 | ppl   283.25\n",
      "| epoch   5 |   300/ 2330 batches | lr 4.00 | loss  5.65 | ppl   283.72\n",
      "| epoch   5 |   400/ 2330 batches | lr 4.00 | loss  5.69 | ppl   295.19\n",
      "| epoch   5 |   500/ 2330 batches | lr 4.00 | loss  5.65 | ppl   283.48\n",
      "| epoch   5 |   600/ 2330 batches | lr 4.00 | loss  5.63 | ppl   277.46\n",
      "| epoch   5 |   700/ 2330 batches | lr 4.00 | loss  5.62 | ppl   274.88\n",
      "| epoch   5 |   800/ 2330 batches | lr 4.00 | loss  5.69 | ppl   295.79\n",
      "| epoch   5 |   900/ 2330 batches | lr 4.00 | loss  5.64 | ppl   280.67\n",
      "| epoch   5 |  1000/ 2330 batches | lr 4.00 | loss  5.63 | ppl   279.62\n",
      "| epoch   5 |  1100/ 2330 batches | lr 4.00 | loss  5.69 | ppl   294.93\n",
      "| epoch   5 |  1200/ 2330 batches | lr 4.00 | loss  5.68 | ppl   291.58\n",
      "| epoch   5 |  1300/ 2330 batches | lr 4.00 | loss  5.64 | ppl   281.73\n",
      "| epoch   5 |  1400/ 2330 batches | lr 4.00 | loss  5.64 | ppl   281.47\n",
      "| epoch   5 |  1500/ 2330 batches | lr 4.00 | loss  5.66 | ppl   287.04\n",
      "| epoch   5 |  1600/ 2330 batches | lr 4.00 | loss  5.63 | ppl   278.39\n",
      "| epoch   5 |  1700/ 2330 batches | lr 4.00 | loss  5.64 | ppl   282.61\n",
      "| epoch   5 |  1800/ 2330 batches | lr 4.00 | loss  5.60 | ppl   271.37\n",
      "| epoch   5 |  1900/ 2330 batches | lr 4.00 | loss  5.62 | ppl   275.38\n",
      "| epoch   5 |  2000/ 2330 batches | lr 4.00 | loss  5.63 | ppl   277.81\n",
      "| epoch   5 |  2100/ 2330 batches | lr 4.00 | loss  5.67 | ppl   288.78\n",
      "| epoch   5 |  2200/ 2330 batches | lr 4.00 | loss  5.63 | ppl   279.68\n",
      "| epoch   5 |  2300/ 2330 batches | lr 4.00 | loss  5.59 | ppl   268.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  5.10 | valid ppl   163.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " is notably at provide k . hindman only the 1862 from early 1983 by cardinal technology found at a human \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(20), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(20), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075.txt', 'w') as outf:\n",
    "    outf.write(t075)\n",
    "with open('./generated1.txt', 'w') as outf:\n",
    "    outf.write(t1)\n",
    "with open('./generated15.txt', 'w') as outf:\n",
    "    outf.write(t15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
